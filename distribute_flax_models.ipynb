{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data parallel training with JAX\n",
    "\n",
    "This notebook compares two methods (pmap, data sharding + jit) for data parallel training with JAX in combination with a simple flax neural network model. \n",
    "\n",
    "It was run on a compute node with 2 Quadro RTX 6000 GPUs with jax 0.4.31 and flax 0.8.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "\n",
    "import flax.linen as nn\n",
    "from flax import jax_utils\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CudaDevice(id=0), CudaDevice(id=1)]\n"
     ]
    }
   ],
   "source": [
    "print(f'JAX devices: {jax.devices()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple multilayer perceptron\n",
    "class MLP(nn.Module):\n",
    "  n_layers: int\n",
    "  hidden_dim: int\n",
    "  output_dim: int\n",
    "  def setup(self):\n",
    "    self.layers = [nn.Dense(self.hidden_dim) for _ in range(self.n_layers)]\n",
    "    self.output_layer = nn.Dense(self.output_dim)\n",
    "  def __call__(self, x: jax.Array):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "      x = nn.relu(x)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available modes:\n",
    "'jit_single' Use single GPU, jit train function\n",
    "\n",
    "'jit_multi': Use multiple GPUs, split data overs GPUs, copy trainig parameters to all GPUs, jit train function. Recommended option\n",
    "\n",
    "'pmap': Use multiple GPUs, reshape data with new axis of size n_gpus, replicate trainig parameters to all GPUs, pmap train function. Depreceated but sill works well\n",
    "\n",
    "\n",
    "### Performance:\n",
    "pmap and jit_multi performs similar, jit_single is slower for large models/batchsizes and faster otherwise\n",
    "\n",
    "For the hyperparameters below one training steps takes: jit_single: 1.2s, jit_multi: 0.7s, pmap: 0.7s\n",
    "\n",
    "For batchsize 256 * 54, one training steps takes: jit_single: out of memory, jit_multi: 1.3s, pmap: 1.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:05:31.499596: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# for mode description, see above\n",
    "mode = 'jit_single' \n",
    "# create a large model\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "key = jax.random.PRNGKey(0)\n",
    "batchsize=256 * 27 # if jit_multi/pmap, can handle up to * 54\n",
    "input_dim=1024\n",
    "hidden_dim = 2056 \n",
    "n_layers = 90 # if jit_multi/pmap, can handle up to 130\n",
    "batch_data = jnp.ones((batchsize, input_dim), dtype=jnp.float32)\n",
    "label_data = jnp.ones((batchsize, 10), dtype=jnp.int32)\n",
    "\n",
    "model = MLP(n_layers=n_layers, hidden_dim=hidden_dim, output_dim=10)\n",
    "\n",
    "def init_fn(k, x, model, optimizer):\n",
    "  variables = model.init(k, x) # Initialize the model.\n",
    "  state = TrainState.create( # Create a `TrainState`.\n",
    "    apply_fn=model.apply,\n",
    "    params=variables,\n",
    "    tx=optimizer)\n",
    "  return state\n",
    "\n",
    "initialized_state = init_fn(key, batch_data, model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_step(state, x, y):\n",
    "  def loss_fn(params, x, y):\n",
    "    y_pred = state.apply_fn(params, x)\n",
    "    return jnp.mean((y_pred - y) ** 2)\n",
    "  loss_grad_fn = jax.value_and_grad(loss_fn)\n",
    "  loss, grads = loss_grad_fn(state.params, x, y)\n",
    "  if mode == 'pmap':\n",
    "    grads = jax.lax.pmean(grads, axis_name='device')\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'jit_multi':\n",
    "  # Mesh sharding splits data onto devices along the 'data' axis\n",
    "  mesh = Mesh(devices=np.array(jax.devices()),\n",
    "              axis_names=('data'))\n",
    "\n",
    "  def mesh_sharding(pspec: PartitionSpec) -> NamedSharding:\n",
    "    return NamedSharding(mesh, pspec)\n",
    "\n",
    "  # data sharding splits data onto devices along the 'data' axis\n",
    "  data_sharding = mesh_sharding(PartitionSpec('data'))\n",
    "  batch_data = jax.device_put(batch_data, data_sharding)\n",
    "  label_data = jax.device_put(label_data, data_sharding)\n",
    "\n",
    "  # this copies the state to all devices\n",
    "  initialized_state = jax.device_put(initialized_state, mesh_sharding(()))\n",
    "\n",
    "  train_step_compiled = jax.jit(train_step)\n",
    "\n",
    "elif mode == 'pmap':\n",
    "  # replicate the initialized state across all devices\n",
    "  initialized_state  = jax_utils.replicate(initialized_state)\n",
    "  # extra axis of size jax.device_count() for data parallelism\n",
    "  batch_data = jnp.reshape(batch_data, (jax.device_count(), batchsize // jax.device_count(), input_dim))\n",
    "  label_data = jnp.reshape(label_data, (jax.device_count(), batchsize // jax.device_count(), 10))\n",
    "  # pmap the train_step function\n",
    "  train_step_compiled = jax.pmap(train_step, axis_name=\"device\", in_axes=(0, 0, 0))\n",
    "\n",
    "elif mode == 'jit_single':\n",
    "  train_step_compiled = jax.jit(train_step)\n",
    "\n",
    "else:\n",
    "  raise ValueError(f'Unknown mode: {mode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:05:39.326527: W external/xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 8.78GiB (9431760227 bytes) by rematerialization; only reduced to 9.10GiB (9772517936 bytes), down from 13.81GiB (14831659588 bytes) originally\n",
      "2024-12-05 12:05:50.755953: W external/xla/xla/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.12MiB (rounded to 16908544)requested by op \n",
      "2024-12-05 12:05:50.756461: W external/xla/xla/tsl/framework/bfc_allocator.cc:494] ****************************************************************************************************\n",
      "E1205 12:05:50.756764   10644 pjrt_stream_executor_client.cc:3067] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16908544 bytes.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16908544 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_compiled\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitialized_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/conda-envs/jax/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1248\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1246\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1248\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1251\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16908544 bytes."
     ]
    }
   ],
   "source": [
    "state, loss = train_step_compiled(initialized_state, batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2056)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.opt_state[0].mu['params']['layers_0']['kernel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                      GPU 0                       </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                      \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mGPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m                       \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  GPU 0  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mGPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if mode == 'pmap':\n",
    "  # we can see that the model is copied across all devices\n",
    "  jax.debug.visualize_array_sharding(state.opt_state[0].mu['params']['layers_0']['kernel'][0])\n",
    "  jax.debug.visualize_array_sharding(state.opt_state[0].mu['params']['layers_0']['kernel'][1])\n",
    "  jax.debug.visualize_array_sharding(batch_data[0])\n",
    "  jax.debug.visualize_array_sharding(batch_data[1])\n",
    "else:\n",
    "  # we can see that the model is copied across all devices and the data is split if multiprocessing is used\n",
    "  jax.debug.visualize_array_sharding(state.opt_state[0].mu['params']['layers_0']['kernel'])\n",
    "  jax.debug.visualize_array_sharding(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 s ± 2.84 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "new_state, loss = jax.block_until_ready(train_step_compiled(initialized_state, batch_data, label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
